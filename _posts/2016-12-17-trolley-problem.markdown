---
layout: post
title:  "Focusing on Self-Driving Trolley Car Problem Misses the Point."
date:   2016-12-17 00:00:00 -0400
categories: robots autonomy self-driving-cars trolley-problem uber tesla apple
author: AMB
published: true
robots_rating: 1
---

*TL;DR: The Trolley Problem is an interesting philosophical problem to explore ethical dilemmas. However, recent media focus on the Trolley Problem is misplaced. The imaginary self-driving cars that decide who lives or dies distract from today's very real ethical problems concerning self-driving cars and the tech companies that build them.*

# An Old Thought Experiment Reborn

<iframe width="560" height="315" src="https://www.youtube.com/embed/bOpf6KcWYyw" frameborder="0" allowfullscreen></iframe>


The Trolley Problem is a thought experiment designed to tease out the moral consequences of taking an action that will save some lives, but doom others.  It was originally authored by philosophers trying to study the morality of abortion in the 1960's. It was not authored with self-driving cars in mind.  However, some MIT researchers rewrote the problem to use self-driving cars as the arbiters of life and death, and named it 'The Moral Machine.' They then published website-based quiz that uses the thought experiment to tease out what [people's moral stances on such actions actually are](http://moralmachine.mit.edu/).

The MIT research project site lists more than 2 dozen 'selected media' that wrote articles about the experiment, including: the New York Times, the Guardian, The Times, Newsweek, PBS,  Scientific American, Popular Science,  TechCrunch, New Scientist, Wired, the Huffington Post, and many more. Philosophy researchers should be congratulated on attracting media attention to their research.  

However, this experiment relies on the acceptance of a fictional premise: that a self-driving car can be programmed with software that takes a moral position on who to save in an emergency.  We run into a problem when the media ignores the fact that this **is not possible** with present technology. The articles written wave away that fact with statements like "In 5 or 10 years cars will do X, Y, and Z." After all, everyone loves a story about [murderous future technology from a giant evil corporation](https://www.youtube.com/watch?v=-fN82upbGPo) - straight from science fiction to real life! 

# Death By Numbers

What with all that media attention, this future ethics problem must represent a real ethics problem today, right? Are the thousands of people dead each year from car crashes the direct result of other people's decisions to run them down to avoid possible death themselves? Well, [hit-and-runs](http://www.iii.org/issue-update/auto-crashes) might be. But the rest are called **accidents** for a reason.  When we break down just how many times a year a future self-driving car will be faced with this kind of awful choice, the results tell an unfortunate story about human drivers, but not definitely not one that matches the kind of media attention received by the Moral Machine. 

The [National Highway Traffic Safety Administration](https://crashstats.nhtsa.dot.gov/#/) collects and summarizes the statistics on all traffic fatalities in the United States. For 2015 (the latest year full data is available), fatalities sit at [1.12 per 100 million vehicle miles travelled](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812349) and 35,092 people. The relevant question here is this: how many of those fatal accidents, if faced by a self-driving car, could result in a moral dilemma worthy of media attention? 

We can answer this question by removing factors that would not contribute to our hypothetical situation. An easy place to start is with alcohol. Alcohol consumption *by the driver* was a factor in [29% of fatal accidents in 2015.](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812357) Let us make the small leap that self-driving cars smart enough to consider moral decisions will eliminate this kind of fatality. We can jump even further and say that in general, for the trolley problem to be relevant, we have to eliminate most kinds of driver (drunk or not) error from the decision making process because the trolley problem is about a **deliberate decision in the face of an 'already' bad situation**, not about **making an error that causes a bad situation**. Which brings us to the following breakdown on car crash causes from an [in-depth 2005 NHTSA study](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811059):

**94% of fatal car crashes are caused by driver error.**
**4% are caused by environment or vehicle failure.**
**2% are caused by something else.**

In other words, the ethics problem of what to do if something bad **just happens** isn't relevant between 94 to 98% of the time. Somewhere between 0% and 4% of the time (since the self-driving car could conceiveably be better at controlling vehicle or environmental failure gracefully), it doesn't matter what decision the car makes because the car isn't in control- the hailstorm or flood is.  94% of the time, the driver makes a mistake. **In other words, the driver is the cause of the failed trolley**. Only 2 to 6% of car crashes exist in a world where the self-driving car can choose who dies. The other 94% exist in a world where such an amazing self-driving car **should have prevented the accident**. 

Side note: The Moral Machine does not provide a setting where the self-driving car makes a mistake, or has too high of a blood alcohol content. Because that's not what the MIT researchers were testing. 

# The President Misses the Point
>"...what are the values that we’re going to embed in the cars? There are gonna be a bunch of choices that you have to make, the classic problem being: If the car is driving, you can swerve to avoid hitting a pedestrian, but then you might hit a wall and kill yourself. It’s a moral decision, and who’s setting up those rules?"
>-[President Barack Obama in Wired interview, October 2016](https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/)

The all time high of "Missing the Point" occurred in October 2016, with *Wired* highlighting President Obama's response to the problem in an interview excerpt. To be fair, the President was interviewed after a visit to the MIT Media Lab, originator of the Moral Machine. It makes sense that he responded to the hype, but that doesn't make the reporting less of a problem. 

Study of the trolley car decision is important to the field of ethics, but the media seized on this problem and the Internet proliferated it for the same reason college roommates like to talk about the meaning of life after a few rounds of beer:  it's an easily-visualized philosophical problem that uses a colorful metaphor with no official wrong answer.  Plus, it has a web-site with cartoon homeless people you can choose to run over with self-driving cars! (And I really, really wish I was being sarcastic there.) 

The philosophy problem was introduced as a relevant news topic as if self-driving cars were capable of making that decision now, and **the tech is not there yet.**  Having a nationwide argument about it camouflages the very real ethical dilemmas (related to self-driving cars and not) that both the American government and companies like Google, Apple, Uber and Tesla have right now. Among them: 
    
 - Companies accusing government regulators of being 'anti-technology' because their safety concerns might hurt corporate profits.
 - Local governments having to balance public safety in the testing of cars versus the economic benefits of being known as a tech-friendly town. 
 - Uber's not paying existing drivers [anything resembling a living wage](https://www.linkedin.com/pulse/hidden-costs-driving-uber-john-mcdermott) and attempting to replace these drivers with self-driving cars
 - Apple and Tesla's potential sourcing of  battery materials from  supplies from [cobalt mines worked by starving child laborers.](https://www.washingtonpost.com/graphics/business/batteries/congo-cobalt-mining-for-lithium-ion-battery/)
 - Google using a favorable interpretation of legal requirements to [under-report how many times its test drivers took over from a self-driving car](https://www.theguardian.com/technology/2016/jan/12/google-self-driving-cars-mistakes-data-reports). 
 - How regulators will handle legislating a [potentially impossible burden of proof](http://www.rand.org/blog/2016/05/why-its-nearly-impossible-to-prove-self-driving-cars.html) that would show self-driving cars are safe enough.
 
Focusing widespread media coverage on the Trolley Problem ignores real ethical dilemmas in favor of fantastical ones. At this point in time, informed technologists,  media authors among them, have a responsibility to look at the real issues. Ones that might not have a website with cute illustration.



# Data Sources 
[FARS Fatality Data 2015](ftp://ftp.nhtsa.dot.gov/fars/2015/National/)









