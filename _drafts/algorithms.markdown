---
layout: post
title: '5 Landmark Algorithms that Made Robots Safer'
date: '2016-09-19 19:00:00 -0400'
categories: machine-learning research robotics safety
author: AMB
published: false
---

*TL;DR: We want robots that are fast, smart, and safe.   In 2016, we're closer than ever to achieving that goal. Self-driving cars, package-delivering drones, and robotic vacuum cleaners are all possible because of increased hardware availability, but also because of some particularly smart thinking.  This article covers the historic biggest leaps forward in algorithms for mobile robots. *



##1960s:  NASA uses Digital Image Processing for Mars Photos
One of the most enduring and popular outputs of the robots that NASA sends to space is the beautiful images they send back to the public, but early projects had a problem.  Analog hardware settings for things like exposure time and photographic sensitivity had to be set in advance, and couldn't be easily changed once a vehicle was in space.  Robert Nathan of NASA's Jet Propulsion Laboratory [convinced early mission project leaders](http://history.nasa.gov/computers/Ch9-3.html)  to add Digital Image Processing to the Mariner  and Surveyor programs in the mid-1960s.   

Digital Image Processing is crucial for robotics, and it is so commonplace now that we often don't even recognize it as an engineering achievement.  DIP at its core is 'smart mapping' in a photo- taking a gray image and stretching the colors to the lightest white and the blackest black to allow our eyes to see more information in an image.   DIP is what allows users to fill Pinterest with moody self-portraits,  but it also picks out cancer cells from normal cells in imaging equipment, and lets us see very small differences in temperature or elevation very clearly.   Without the algorithms to change the map of input pixels to output pixels, we would never see stunning images like this: 

![Notice how this isn't what YOU see when you stare at the sun...](http://apod.nasa.gov/apod/image/1609/Filaprom_Lawrence_960.jpg)http://apod.nasa.gov/apod/astropix.html

We also wouldn't be able to make our own pictures look better either...
![enter image description here](http://lestaylorphoto.com/wp-content/uploads/2015/07/Before-After-1400W%28pp_w768_h256%29.jpg)
Now imagine this... on the moon. 

Once we could map image to image, DIP also expanded the capabilities of *other* types of sensors. If robots can map from camera to camera, then they can map from sonar to camera, or LIDAR to camera, or any other number of sensors.   

##1970s
##1980s
##1990s
##2000s
##2010s

